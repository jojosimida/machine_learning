{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('D:/Program/dataset/Spooky_Author_Identification/train.csv')\n",
    "test_df = pd.read_csv('D:/Program/dataset/Spooky_Author_Identification/test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTokenization - Segregation of the text into its individual constitutent words.\\n\\nStopwords - Throw away any words that occur too frequently as its frequency of occurrence will not be \\n            useful in helping detecting relevant texts. (as an aside also consider throwing away words \\n            that occur very infrequently).\\n            \\nStemming - combine variants of words into a single parent word that still conveys the same meaning\\n\\nVectorization - Converting text into vector format. One of the simplest is the famous bag-of-words approach, \\n                where you create a matrix (for each document or text in the corpus). In the simplest form, \\n                this matrix stores word frequencies (word counts) and is oft referred to as vectorization of the raw text.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenization - Segregation of the text into its individual constitutent words.\n",
    "\n",
    "Stopwords - Throw away any words that occur too frequently as its frequency of occurrence will not be \n",
    "            useful in helping detecting relevant texts. (as an aside also consider throwing away words \n",
    "            that occur very infrequently).\n",
    "            \n",
    "Stemming - combine variants of words into a single parent word that still conveys the same meaning\n",
    "\n",
    "Vectorization - Converting text into vector format. One of the simplest is the famous bag-of-words approach, \n",
    "                where you create a matrix (for each document or text in the corpus). In the simplest form, \n",
    "                this matrix stores word frequencies (word counts) and is oft referred to as vectorization of the raw text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "stemmer = nltk.stem.SnowballStemmer(language='english')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords = set(stopwords)\n",
    "stopwords.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "\n",
    "sentences = []    \n",
    "    \n",
    "for text in train_df['text']:\n",
    "    tokens = []\n",
    "    \n",
    "    # Segregation of the text into its individual constitutent words.\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token.lower() not in stopwords:\n",
    "            tokens.append(stemmer.stem(token.lower()))\n",
    "    sentences.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'process howev afford mean ascertain dimens dungeon might make circuit return point whenc set without awar fact perfect uniform seem wall'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "for text in test_df['text']:\n",
    "    tokens = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token.lower() not in stopwords:\n",
    "            tokens.append(stemmer.stem(token.lower()))\n",
    "    test_sentences.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(sentences + test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for vectorizing texts, or/and turning texts into sequences\n",
    "sequences = tokenizer.texts_to_matrix(sentences,\"tfidf\")\n",
    "\n",
    "# pad the sequence to maxlen \n",
    "# if the sequence shorter than the maxlen, pad with 0 at the end\n",
    "sequences = pad_sequences(sequences, maxlen=maxlen,padding='post', truncating='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_matrix(test_sentences,\"tfidf\")\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=maxlen,padding='post', truncating='post')\n",
    "\n",
    "\n",
    "# Pickle is used for serializing and de-serializing a Python object structure.\n",
    "# Any object in python can be pickled so that it can be saved on disk. \n",
    "with open('keras_input_train.pkl', 'wb') as f:\n",
    "    pickle.dump(sequences, f)\n",
    "with open('keras_input_test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_sequences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping words (str) to their rank/index (int) \n",
    "vocab = tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
